# MNIST Handwritten Digit Recognizer

Building a neural network using NumPy was done to give myself a deeper understanding of how the neural networks learn using gradient descsent and backpropagation. The neural network bulit with Numpy was built alongside reading: http://neuralnetworksanddeeplearning.com/index.html by Michael Nielsen.  
  
I also wanted to learn how to use a deep learning library such as TensorFlow to build neural networks. Using Tensorflow allowed me to further explore and understand aspects of model creation such as how different layers, activation functions, loss functions, and optimizers affect the outcome of a network.

## Resources  
  
Activtaion functions: https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/  
Dropout: https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386  
Feature scaling: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/  
Loss functions: https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/  
Convolutional Networks: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53  
https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8  
Overfitting: https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a  
Dropout: https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386  
https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/
